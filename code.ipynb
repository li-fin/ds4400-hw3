{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c428455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.metrics import (mean_squared_error, r2_score, confusion_matrix, \n",
    "    accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74282b3d",
   "metadata": {},
   "source": [
    "git steps:\n",
    "\n",
    "`git add .`\n",
    "\n",
    "`git commit -m \"message\"`\n",
    "\n",
    "`git push`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f4209",
   "metadata": {},
   "source": [
    "written answers are in PDF submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b0758",
   "metadata": {},
   "source": [
    "# Problem 1 [Regularization]\n",
    "c. simulate N = 1000 values of random variable X, distributed uniformly on interval [-2, 2]. simulate the values of random variable Y = 1 + 2X + e, where e is drawn from a gaussian distribution N(0, 2).\n",
    "\n",
    "fit this data with linear regression, and also with ridge regression for different values of lambda {1, 10, 100, 1000, 10000}. print the slope, MSE, and R^2 statistics for each case and write down some observations. what happens as the regularization parameter lambda increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46bdd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression\n",
      "slope: 1.95\n",
      "MSE: 1.95\n",
      "R^2: 0.73\n",
      "\n",
      "ridge regression (lambda = 1)\n",
      "slope: 1.94\n",
      "MSE: 1.95\n",
      "R^2: 0.73\n",
      "\n",
      "ridge regression (lambda = 10)\n",
      "slope: 1.93\n",
      "MSE: 1.95\n",
      "R^2: 0.73\n",
      "\n",
      "ridge regression (lambda = 100)\n",
      "slope: 1.81\n",
      "MSE: 1.97\n",
      "R^2: 0.72\n",
      "\n",
      "ridge regression (lambda = 1000)\n",
      "slope: 1.12\n",
      "MSE: 2.87\n",
      "R^2: 0.6\n",
      "\n",
      "ridge regression (lambda = 10000)\n",
      "slope: 0.23\n",
      "MSE: 5.95\n",
      "R^2: 0.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# simulate data\n",
    "N = 1000 # 1000 values\n",
    "X = np.random.uniform(-2, 2, size=(N, 1)) # RV X distributed uniformly on interval [-2, 2]\n",
    "e = np.random.normal(0, np.sqrt(2), size=N) # e from gaussian distribution N(0,2)\n",
    "y = 1 + 2 * X.flatten() + e # RV Y (given equation)\n",
    "\n",
    "# linear regression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "y_pred_lin = linreg.predict(X)\n",
    "\n",
    "print(\"linear regression\")\n",
    "print(\"slope:\", round(linreg.coef_[0], 2))\n",
    "print(\"MSE:\", round(mean_squared_error(y, y_pred_lin), 2))\n",
    "print(\"R^2:\", round(r2_score(y, y_pred_lin), 2))\n",
    "print()\n",
    "\n",
    "# ridge regression w/ different values of lambda\n",
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=True)\n",
    "    ridge.fit(X, y)\n",
    "    y_pred_ridge = ridge.predict(X)\n",
    "    \n",
    "    print(f\"ridge regression (lambda = {lam})\")\n",
    "    print(\"slope:\", round(ridge.coef_[0], 2))\n",
    "    print(\"MSE:\", round(mean_squared_error(y, y_pred_ridge), 2))\n",
    "    print(\"R^2:\", round(r2_score(y, y_pred_ridge), 2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be3e0b",
   "metadata": {},
   "source": [
    "# Problem 2 [Programming: Logistic Regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e33d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in datasets\n",
    "airlines = pd.read_csv(\"airlines.csv\")\n",
    "airports = pd.read_csv(\"airports.csv\")\n",
    "columns = pd.read_csv(\"columns.csv\")\n",
    "flights = pd.read_csv(\"flights.csv\")\n",
    "\n",
    "feature = 'ARRIVAL_DELAY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d1a18",
   "metadata": {},
   "source": [
    "first split the original data into 75% for training and 25% for testing. choose the training set at random. then use StandardScaler to normalize the features of the training set and train a logistic regression model on this normalized training set. apply the same transformation to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7bc1451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target\n",
    "X = flights.drop(columns=feature)\n",
    "y = flights[feature]\n",
    "\n",
    "# split into training and testing (75:25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # fit on train\n",
    "X_test_scaled = scaler.transform(X_test) # apply to test\n",
    "\n",
    "# train logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predictions using test set\n",
    "y_pred = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d21ded",
   "metadata": {},
   "source": [
    "a. output the following on the testing set:\n",
    "- confusion matrix\n",
    "- true positives, false positives, true negatives, false negatives\n",
    "- accuracy, error\n",
    "- precision, recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b0f8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[707 544]\n",
      " [505 744]]\n",
      "\n",
      "true positives (TP): 744\n",
      "false positives (FP): 544\n",
      "true negatives (TN): 707\n",
      "false negatives (FN): 505\n",
      "\n",
      "accuracy: 0.58\n",
      "error: 0.42\n",
      "precision: 0.58\n",
      "recall: 0.6\n",
      "f1 score: 0.59\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix and TP, FP, TN, FP\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "print(\"true positives (TP):\", tp)\n",
    "print(\"false positives (FP):\", fp)\n",
    "print(\"true negatives (TN):\", tn)\n",
    "print(\"false negatives (FN):\", fn)\n",
    "print()\n",
    "\n",
    "# other metrics\n",
    "print(\"accuracy:\", round(accuracy_score(y_test, y_pred), 2))\n",
    "print(\"error:\", round(1 - accuracy_score(y_test, y_pred), 2))\n",
    "print(\"precision:\", round(precision_score(y_test, y_pred), 2))\n",
    "print(\"recall:\", round(recall_score(y_test, y_pred), 2))\n",
    "print(\"f1 score:\", round(f1_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c718dac",
   "metadata": {},
   "source": [
    "b. print the coefficients of the features in the model. which feature contributes most to the prediction? what are the top three features that are most positively correlated with the class? similarly, what are the top three features that are most negatively correlated with the class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89b635c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     feature  coefficient\n",
      "0                      MONTH    -0.115063\n",
      "1                DAY_OF_WEEK    -0.052369\n",
      "2                   DISTANCE    -0.021925\n",
      "3        SCHEDULED_DEPARTURE     0.437035\n",
      "4                 AIRLINE_AS    -0.008503\n",
      "..                       ...          ...\n",
      "654  DESTINATION_AIRPORT_WRG     0.197158\n",
      "655  DESTINATION_AIRPORT_WYS    -0.071766\n",
      "656  DESTINATION_AIRPORT_XNA    -0.053666\n",
      "657  DESTINATION_AIRPORT_YAK     0.000000\n",
      "658  DESTINATION_AIRPORT_YUM    -0.146941\n",
      "\n",
      "[659 rows x 2 columns]\n",
      "\n",
      "feature that contributes the most:\n",
      "feature        SCHEDULED_DEPARTURE\n",
      "coefficient               0.437035\n",
      "Name: 3, dtype: object\n",
      "\n",
      "top three positively correlated features:\n",
      "                     feature  coefficient\n",
      "3        SCHEDULED_DEPARTURE     0.437035\n",
      "654  DESTINATION_AIRPORT_WRG     0.197158\n",
      "95        ORIGIN_AIRPORT_CWA     0.152991\n",
      "\n",
      "top 3 negatively correlated features:\n",
      "                     feature  coefficient\n",
      "338  DESTINATION_AIRPORT_ABI    -0.225254\n",
      "133       ORIGIN_AIRPORT_FSM    -0.171173\n",
      "524  DESTINATION_AIRPORT_LRD    -0.158592\n"
     ]
    }
   ],
   "source": [
    "# get feature names from columns.csv\n",
    "feature_names = columns.iloc[:, 0].values\n",
    "feature_names = feature_names[feature_names != feature]\n",
    "\n",
    "# get coefficients\n",
    "coefs = logreg.coef_.flatten()\n",
    "\n",
    "# store in dataframe\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coefficient\": coefs\n",
    "})\n",
    "\n",
    "# results\n",
    "print(coef_df)\n",
    "print()\n",
    "\n",
    "print(\"feature that contributes the most:\")\n",
    "print(coef_df.loc[coef_df.coefficient.abs().idxmax()])\n",
    "print()\n",
    "\n",
    "print(\"top three positively correlated features:\")\n",
    "print(coef_df.sort_values(\"coefficient\", ascending=False).head(3))\n",
    "print()\n",
    "\n",
    "print(\"top 3 negatively correlated features:\")\n",
    "print(coef_df.sort_values(\"coefficient\").head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec1d2d",
   "metadata": {},
   "source": [
    "c. vary the decision threshold T {0.25, 0.5, 0.75, 0.9} and report for each value the model accuracy, precision, and recall. comment on how these metrics vary with the choice of threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00347803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy  precision  recall\n",
       "0       0.25      0.53       0.51    0.96\n",
       "1       0.50      0.58       0.58    0.60\n",
       "2       0.75      0.52       0.60    0.09\n",
       "3       0.90      0.51       0.65    0.02"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted probabilities for the positive class\n",
    "y_prob = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "thresholds = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "results = []\n",
    "\n",
    "for T in thresholds:\n",
    "    y_pred_T = (y_prob >= T).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        \"threshold\": T,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_T),\n",
    "        \"precision\": precision_score(y_test, y_pred_T),\n",
    "        \"recall\": recall_score(y_test, y_pred_T)\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "round(results, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631cd14",
   "metadata": {},
   "source": [
    "# Problem 3 [Programming: Gradient Descent for Logistic Regression]\n",
    "\n",
    "use your implementation of gradient descent from HW2 and adapt it for logistic regression. you can use the same training and testing split from problem 2.\n",
    "\n",
    "take 3 values of the learning rate (0.1, 0.01, 001) and report the cross-entropy loss objective after 10, 50, and 100 iterations. at 100 iterations, report the accuracy, precision, recall, and F1 score for the 3 learning rates, and compare with the metrics from problem 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "503f65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    eps = 1e-15\n",
    "    y_hat = np.clip(y_hat, eps, 1-eps)\n",
    "    return -np.mean(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "\n",
    "def gradient_descent_logreg(xb, y, alpha, num_iters):\n",
    "    n, d = xb.shape\n",
    "    theta = np.zeros((d, 1))\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        z = xb @ theta\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        grad = (1/n) * (xb.T @ (y_hat-y))\n",
    "        theta = theta - alpha * grad\n",
    "\n",
    "        loss = cross_entropy_loss(y, y_hat)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return theta, losses\n",
    "\n",
    "def predict_proba(xb, theta):\n",
    "    return sigmoid(xb @ theta).flatten()\n",
    "\n",
    "def predict_labels(xb, theta, threshold=0.5):\n",
    "    return (predict_proba(xb, theta) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d60d594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning rate</th>\n",
       "      <th># iterations</th>\n",
       "      <th>cross-entropy loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>50</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning rate  # iterations  cross-entropy loss\n",
       "0          0.100            10               0.667\n",
       "1          0.100            50               0.630\n",
       "2          0.100           100               0.622\n",
       "3          0.010            10               0.690\n",
       "4          0.010            50               0.677\n",
       "5          0.010           100               0.665\n",
       "6          0.001            10               0.693\n",
       "7          0.001            50               0.691\n",
       "8          0.001           100               0.689"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# add intercept column\n",
    "xb_train = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "xb_test = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "iterations = [10, 50, 100]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    theta, losses = gradient_descent_logreg(xb_train, y_train, alpha, max(iterations))\n",
    "    \n",
    "    for it in iterations:\n",
    "        rows.append([alpha,it,losses[it - 1]  ])\n",
    "\n",
    "gd_log_results = pd.DataFrame(rows,columns=[\"learning rate\", \"# iterations\", \"cross-entropy loss\"])\n",
    "\n",
    "round(gd_log_results, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f52a08d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning rate  accuracy  precision  recall  f1 score\n",
       "0          0.100     0.643      0.640   0.642     0.641\n",
       "1          0.010     0.635      0.630   0.642     0.636\n",
       "2          0.001     0.632      0.627   0.638     0.633"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics at 100 iterations\n",
    "\n",
    "metric_rows = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    theta, _ = gradient_descent_logreg(xb_train, y_train, alpha, 100)\n",
    "\n",
    "    yhat_train = predict_labels(xb_train, theta)\n",
    "\n",
    "    metric_rows.append([\n",
    "        alpha,\n",
    "        accuracy_score(y_train, yhat_train),\n",
    "        precision_score(y_train, yhat_train),\n",
    "        recall_score(y_train, yhat_train),\n",
    "        f1_score(y_train, yhat_train)\n",
    "    ])\n",
    "\n",
    "gd_log_metrics = pd.DataFrame(metric_rows, columns=[\n",
    "    \"learning rate\", \"accuracy\", \"precision\", \"recall\", \"f1 score\"\n",
    "])\n",
    "\n",
    "round(gd_log_metrics, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
